<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>ML(6)_线性回归 | ⎛⎝&gt;⏝⏝&lt;⎛⎝</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模 线性回归输出: h(x) &#x3D; w1x1 + w2x2 + ··· + b &#x3D; W^T * X, W&#x3D;(w1,w2,…),X&#x3D;(x1,x2,…) 目标求参数矩阵W与偏置(截距)b,先假定一组参数,再与真实值比对,计算损失,调参让误差缩小  损失函数: f(θ) &#x3D; Σ">
<meta property="og:type" content="article">
<meta property="og:title" content="ML(6)_线性回归">
<meta property="og:url" content="https://by2095163684.github.io/2024/08/02/ML-6/index.html">
<meta property="og:site_name" content="⎛⎝&gt;⏝⏝&lt;⎛⎝">
<meta property="og:description" content="利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模 线性回归输出: h(x) &#x3D; w1x1 + w2x2 + ··· + b &#x3D; W^T * X, W&#x3D;(w1,w2,…),X&#x3D;(x1,x2,…) 目标求参数矩阵W与偏置(截距)b,先假定一组参数,再与真实值比对,计算损失,调参让误差缩小  损失函数: f(θ) &#x3D; Σ">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-08-02T09:33:27.000Z">
<meta property="article:modified_time" content="2024-08-02T09:34:06.323Z">
<meta property="article:author" content="废、寝、忘、食">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="⎛⎝>⏝⏝<⎛⎝" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">⎛⎝&gt;⏝⏝&lt;⎛⎝</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://BY2095163684.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-ML-6" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/02/ML-6/" class="article-date">
  <time class="dt-published" datetime="2024-08-02T09:33:27.000Z" itemprop="datePublished">2024-08-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      ML(6)_线性回归
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模</p>
<p>线性回归输出: h(x) &#x3D; w1<em>x1 + w2</em>x2 + ··· + b &#x3D; W^T * X, W&#x3D;(w1,w2,…),X&#x3D;(x1,x2,…)</p>
<p>目标求参数矩阵W与偏置(截距)b,先假定一组参数,再与真实值比对,计算损失,调参让误差缩小</p>
<ul>
<li>损失函数: f(θ) &#x3D; Σ [ hw(xi)-yi ]^2, yi为真实值<ul>
<li>优化损失</li>
<li>最小二乘法</li>
</ul>
</li>
<li>优化算法<ol>
<li>正规方程: w &#x3D; (X^T * X)^-1 * (X^T * Y), X特征值矩阵,Y目标值矩阵<ul>
<li>优: 直接求得最好结果</li>
<li>缺: 特征多而复杂时,又慢又求不出</li>
</ul>
</li>
<li>梯度下降<ul>
<li>w0 &#x3D; w0 - α * {d[cost(w0 + w1<em>x1)] &#x2F; dw1}; w1 &#x3D; w1 - α * {d[cost(w1 + w1</em>x1)] &#x2F; dw1}</li>
<li>α学习速率(超参数)</li>
<li>优: 面对庞大任务能得到较好的结果</li>
<li>缺: 刚开始总是不美好的</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="线性回归算法"><a href="#线性回归算法" class="headerlink" title="线性回归算法"></a>线性回归算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br></pre></td></tr></table></figure>
<ul>
<li><code>sklearn.linear_model.LinearRegression(fit_intercept=True)</code><ul>
<li>正规方程优化(data &lt; 100k)</li>
<li>fit_intercept: 是否计算偏置b</li>
<li>LinearRegression.coef_: 回归系数</li>
<li>LinearRegression.intercept_: 偏置b</li>
</ul>
</li>
<li><code>sklearn.linear_model.SGDRegressor(loss=&quot;squared_loss&quot;,fit_intercept=True,max_iter=1000, learning_rate=&quot;invscaling&quot;,eta0=0.01)</code><ul>
<li>max_iter: 最大学习次数</li>
<li>随机梯度下降学习,支持不同loss函数和正则化惩罚来拟合线性回归模型</li>
<li>loss: 损失类型(loss&#x3D;”squared_loss”普通最小二乘法)</li>
<li>fit_intercept: 是否计算偏置b</li>
<li>learning_rate: 学习率填充<ul>
<li>“constant”: eta &#x3D; eta0</li>
<li>“optimal”: 默认eta &#x3D; 1.0 &#x2F; [ alpha * (t+t0) ]</li>
<li>“invscaling”: eta &#x3D; eta0 &#x2F; pow(t,power_t), power_t&#x3D;0.25,存在父类当中</li>
</ul>
</li>
<li>SGDRegressor.coef_: 回归系数</li>
<li>SGDRegressor.intercept_: 偏置b</li>
</ul>
</li>
</ul>
<h4 id="回归性能评估-均方误差"><a href="#回归性能评估-均方误差" class="headerlink" title="回归性能评估(均方误差)"></a>回归性能评估(均方误差)</h4><p>MSE &#x3D; Σ (yi - y)^2 &#x2F; m, m个样本,yi预测值,y真实值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.metrics</span><br></pre></td></tr></table></figure>
<ul>
<li><code>sklearn.metrics.mean_squared_error(y_true,y_pred)</code><ul>
<li>均方误差回归损失</li>
<li>y_true真实值,y_pred预测值</li>
<li>返回浮点数结果</li>
</ul>
</li>
</ul>
<h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><p>带L2正则化的线性回归,解决过拟合</p>
<ul>
<li><code>sklearn.linear_model.Ridge(alpha=1.0,fit_intercept=True,solver=&quot;auto&quot;,normalize=False)</code><ul>
<li>alpha: 正则化力度(惩罚项系数)λ , 1&lt;λ&lt;10 or 0&lt;λ&lt;1</li>
<li>solver&#x3D;”auto”: 根据数据自动选择优化方法<ul>
<li>sag: 若数据集,特征都比较大,选择该随机梯度下降优化</li>
</ul>
</li>
<li>normalize: 数据是否标准化</li>
<li>Ridge.coef_: 回归权重</li>
<li>Ridge.intercept_: 回归偏置</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://by2095163684.github.io/2024/08/02/ML-6/" data-id="clzcifdqj00061gve2haaeiop" data-title="ML(6)_线性回归" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/08/02/ML-7/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          ML(7)_逻辑回归
        
      </div>
    </a>
  
  
    <a href="/2024/08/01/ML-5/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">ML(5)_分类算法_2</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">八月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">七月 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/08/02/ML-8/">ML(8)_无监督学习</a>
          </li>
        
          <li>
            <a href="/2024/08/02/ML-7/">ML(7)_逻辑回归</a>
          </li>
        
          <li>
            <a href="/2024/08/02/ML-6/">ML(6)_线性回归</a>
          </li>
        
          <li>
            <a href="/2024/08/01/ML-5/">ML(5)_分类算法_2</a>
          </li>
        
          <li>
            <a href="/2024/07/31/ML-4/">ML(4)_分类算法_1</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 废、寝、忘、食<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>